{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dir = \"/home/vedic/ISI/review_polarity/txt_sentoken/neg/\"\n",
    "pos_dir = \"/home/vedic/ISI/review_polarity/txt_sentoken/pos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_revs = []\n",
    "pos_revs = []\n",
    "\n",
    "for file in os.listdir(neg_dir):\n",
    "    text = open(neg_dir + file, 'r' )\n",
    "    text = text.read()\n",
    "    neg_revs.append(text)\n",
    "    \n",
    "for file in os.listdir(pos_dir):\n",
    "    text = open(pos_dir + file, 'r' )\n",
    "    text = text.read()\n",
    "    pos_revs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev_clean = []\n",
    "pos_rev_clean = []\n",
    "\n",
    "for rev in neg_revs:\n",
    "    neg_rev_clean.append(clean_text(rev))\n",
    "    \n",
    "for rev in pos_revs:\n",
    "    pos_rev_clean.append(clean_text(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ([0 for _ in range(len(neg_rev_clean))] + [1 for _ in range(len(pos_rev_clean))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = neg_rev_clean + pos_rev_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_test, x_train, y_test, y_train = train_test_split(reviews, labels, test_size = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train = pad_sequences(encoded, maxlen=max_length, padding = 'post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation= 'relu' ))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation= 'relu' ))\n",
    "model.add(Dense(1, activation= 'sigmoid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1299, 100)         3583500   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1292, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 646, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20672)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                206730    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 3,815,873\n",
      "Trainable params: 3,815,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.6918 - acc: 0.5056\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 11s 6ms/step - loss: 0.6280 - acc: 0.6100\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 12s 6ms/step - loss: 0.4202 - acc: 0.8967\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.3443 - acc: 0.9578\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 11s 6ms/step - loss: 0.3163 - acc: 0.9678: 1s - loss: 0.3209 - ac\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 11s 6ms/step - loss: 0.2987 - acc: 0.9761: 2s - loss: 0.2991 -\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 12s 6ms/step - loss: 0.2841 - acc: 0.9778\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 12s 6ms/step - loss: 0.2731 - acc: 0.9772\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 12s 6ms/step - loss: 0.2628 - acc: 0.9778\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 12s 6ms/step - loss: 0.2534 - acc: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce29dfcb00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences(x_test)\n",
    "x_test  = pad_sequences(encoded, maxlen=max_length, padding = 'post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5109314346313476, 0.83]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
